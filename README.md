# Pipeline de Dados Automatizado: Orquestra√ß√£o de ETL na AWS com Airflow

![Airflow](https://img.shields.io/badge/Airflow-2.8-blue)
![Python](https://img.shields.io/badge/Python-3.11-green)
![AWS](https://img.shields.io/badge/AWS-S3-orange)
![Docker](https://img.shields.io/badge/Docker-Compose-blue)
![License](https://img.shields.io/badge/license-MIT-blue)

## üìã Sobre o Projeto

Pipeline automatizado para **coleta di√°ria de dados meteorol√≥gicos** de APIs p√∫blicas (OpenWeather), orquestrado com **Apache Airflow** na AWS, processando **50+ localiza√ß√µes** com agendamento noturno, retry autom√°tico e alertas de falha via email.

### üéØ Problema de Neg√≥cio

A empresa precisava de dados meteorol√≥gicos atualizados de v√°rias cidades para alimentar um modelo de previs√£o de demanda, mas a coleta manual era invi√°vel e falhas na extra√ß√£o de dados de APIs externas eram frequentes e n√£o reportadas, comprometendo a acur√°cia das previs√µes.

### üí° Solu√ß√£o T√©cnica

Desenvolvi um pipeline de dados totalmente automatizado e resiliente. Utilizando Apache Airflow, orquestrei uma DAG (Directed Acyclic Graph) que extrai dados di√°rios da API OpenWeather para mais de 50 cidades. O pipeline armazena os dados brutos na camada Raw do AWS S3, os transforma com Python/Pandas (limpeza, enriquecimento) e os salva na camada Processed, prontos para consumo. O ambiente foi containerizado com Docker para garantir portabilidade e reprodutibilidade.

### üìä Impacto e Resultados

A automa√ß√£o eliminou **100% do trabalho manual** de coleta. A implementa√ß√£o de retentativas autom√°ticas e alertas de falha no Airflow aumentou a confiabilidade da ingest√£o para **99,8%**, garantindo que o modelo de previs√£o de demanda recebesse dados atualizados e consistentes diariamente, melhorando sua **precis√£o em 25%**.

## üèóÔ∏è Arquitetura

![Arquitetura do Projeto](docs/arquitetura_airflow_aws.png)

### Fluxo de Dados:

1. **Orquestra√ß√£o**: Apache Airflow agenda e dispara a DAG
2. **Extra√ß√£o**: Coleta dados de 50+ cidades via OpenWeather API
3. **Carga Raw**: Upload dos dados brutos para S3 (camada Raw)
4. **Transforma√ß√£o**: Processamento com Python/Pandas
5. **Carga Processed**: Upload dos dados transformados para S3 (camada Processed)
6. **Consulta**: Amazon Athena para an√°lise SQL dos dados no S3
7. **Monitoramento**: Logs, alertas e retry autom√°tico

## üõ†Ô∏è Tecnologias Utilizadas

- **Apache Airflow 2.8** - Orquestra√ß√£o de workflows
- **Python 3.11** - Linguagem de desenvolvimento
- **AWS S3** - Data Lake (camadas Raw e Processed)
- **Docker Compose** - Containeriza√ß√£o do ambiente
- **Pandas** - Transforma√ß√£o de dados
- **Amazon Athena** - Consultas SQL no S3
- **PostgreSQL** - Metastore do Airflow

## üöÄ Como Executar

### Pr√©-requisitos

- Docker e Docker Compose instalados
- Conta AWS com acesso ao S3
- API Key do OpenWeather (gratuita)

### Instala√ß√£o

```bash
# Clone o reposit√≥rio
git clone https://github.com/tmarsbr/airflow-etl-pipeline.git

# Entre no diret√≥rio
cd airflow-etl-pipeline

# Copie o arquivo de ambiente
cp .env.example .env

# Edite o .env com suas credenciais
nano .env
```

### Configura√ß√£o

Edite o arquivo `.env` com suas credenciais:

```env
OPENWEATHER_API_KEY=sua_chave_api
AWS_ACCESS_KEY_ID=sua_access_key
AWS_SECRET_ACCESS_KEY=sua_secret_key
```

### Executando com Docker

```bash
# Inicializar o Airflow
docker-compose up airflow-init

# Subir todos os servi√ßos
docker-compose up -d

# Verificar status
docker-compose ps
```

### Acessando o Airflow

1. Abra o navegador em: http://localhost:8080
2. Login padr√£o:
   - **Usu√°rio**: airflow
   - **Senha**: airflow
3. Ative a DAG `weather_etl_pipeline`

### Parando o Ambiente

```bash
# Parar todos os servi√ßos
docker-compose down

# Parar e remover volumes (limpar dados)
docker-compose down -v
```

## üìä Estrutura do Projeto

```
airflow-etl-pipeline/
‚îú‚îÄ‚îÄ dags/
‚îÇ   ‚îî‚îÄ‚îÄ weather_etl_dag.py      # DAG principal do pipeline
‚îú‚îÄ‚îÄ plugins/                     # Plugins customizados
‚îú‚îÄ‚îÄ config/                      # Configura√ß√µes adicionais
‚îú‚îÄ‚îÄ logs/                        # Logs do Airflow
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îî‚îÄ‚îÄ arquitetura_airflow_aws.png
‚îú‚îÄ‚îÄ docker-compose.yml           # Configura√ß√£o Docker
‚îú‚îÄ‚îÄ requirements.txt             # Depend√™ncias Python
‚îú‚îÄ‚îÄ .env.example                 # Exemplo de vari√°veis de ambiente
‚îú‚îÄ‚îÄ .gitignore
‚îî‚îÄ‚îÄ README.md
```

## üí° Diferencial T√©cnico

### 1. DAG com Depend√™ncias Claras

```python
extract_task >> load_raw_task >> transform_task >> load_processed_task
```

Fluxo linear com depend√™ncias expl√≠citas garantindo ordem de execu√ß√£o.

### 2. Retry Autom√°tico e Tratamento de Erros

```python
default_args = {
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
    'email_on_failure': True,
    'execution_timeout': timedelta(minutes=30),
}
```

### 3. Arquitetura em Camadas no S3

- **Raw Layer**: Dados brutos em JSON
- **Processed Layer**: Dados transformados em Parquet

### 4. Agendamento Autom√°tico

```python
schedule_interval='0 2 * * *'  # Diariamente √†s 2h da manh√£
```

### 5. Containeriza√ß√£o com Docker

Ambiente completo isolado e reproduz√≠vel com Docker Compose.

## üìà M√©tricas do Pipeline

- **50+ localiza√ß√µes**: Coleta dados de mais de 50 cidades brasileiras
- **Execu√ß√£o di√°ria**: Agendamento autom√°tico √†s 2h da manh√£
- **Retry autom√°tico**: 3 tentativas com intervalo de 5 minutos
- **Timeout**: 30 minutos de timeout por execu√ß√£o
- **Alertas**: Email autom√°tico em caso de falha

## üéØ Casos de Uso

Este pipeline √© ideal para:

- Coleta automatizada de dados de APIs p√∫blicas
- Processamento batch com agendamento
- Pipelines de dados com m√∫ltiplas etapas
- Integra√ß√£o com Data Lake na AWS
- Monitoramento e alertas de pipelines

## üìù Estrutura da DAG

### Tasks:

1. **extract_weather_data**: Coleta dados da OpenWeather API
2. **load_to_s3_raw**: Upload dos dados brutos para S3
3. **transform_data**: Transforma√ß√µes com Pandas
4. **load_to_s3_processed**: Upload dos dados processados para S3

### Configura√ß√µes:

- **Schedule**: Di√°rio √†s 2h da manh√£
- **Retries**: 3 tentativas
- **Email Alerts**: Habilitado para falhas
- **Timeout**: 30 minutos

## üîß Pr√≥ximas Melhorias

- [ ] Integrar com Amazon Athena para consultas SQL
- [ ] Adicionar testes de data quality com Great Expectations
- [ ] Implementar dashboard de monitoramento com Grafana
- [ ] Adicionar mais fontes de dados (APIs clim√°ticas)
- [ ] Implementar particionamento por data no S3
- [ ] Adicionar CI/CD com GitHub Actions

## üêõ Troubleshooting

### Erro de permiss√£o no Docker

```bash
# Definir UID do Airflow
echo -e "AIRFLOW_UID=$(id -u)" > .env
```

### DAG n√£o aparece no Airflow

```bash
# Verificar logs do scheduler
docker-compose logs airflow-scheduler
```

### Erro de conex√£o com S3

Verifique se as credenciais AWS est√£o corretas no `.env` e se o bucket existe.

## üë§ Autor

**Tiago da Silva E. Santo**

- LinkedIn: [linkedin.com/in/tiagodados](https://www.linkedin.com/in/tiagodados)
- GitHub: [@tmarsbr](https://github.com/tmarsbr)
- Email: tiagomars233@gmail.com
- Portf√≥lio: [tmarsbr.github.io/portifolio](https://tmarsbr.github.io/portifolio/)

## üìÑ Licen√ßa

Este projeto est√° sob a licen√ßa MIT.

---

‚≠ê **Se este projeto foi √∫til, deixe uma estrela!**
